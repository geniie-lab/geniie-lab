# Common settings across experiments

## Experiment

- `name`: Name of experiment for logs

```python
my_settings = ExperimentSettings(
    name="my_session_experiment",
```

## Task Description

- `name`: Name of IR task
- `descriptoin`: Description of IR task
- `measurement`: Metrics for retrieval results as defined by `ir_meaures`
- `start_offset`: Start index of rankings (Default: 0)
- `serp_size`: Size of returned results (Default: 10)

### High-Precision Task

```python
    task=TaskDescription(
        name = "High-Precision Retrieval",
        description = "Find the most relevant documents at the top rank for a given search topic from a given document collection using a provided search tool.",
        measurement=[ir_measures.nDCG@10, ir_measures.MRR@10],
        start_offset=0,
        serp_size=10,
    )
```

### High-Recall Task

```python
    task=TaskDescription(
        name="High-Recall Retrieval",
        description="Find as many different relevant documents as possible for a given search topic from a given document collection using a provided search tool.",
        measurement=[ir_measures.nDCG@1000, ir_measures.Recall@1000],
        start_offset=0,
        serp_size=1000,
    )
```

### High-Diversity Task

- Make sure that you have intent-aware datasets to use this setting.

```python
    task=TaskDescription(
        name = "High-Diversity Retrieval",
        description = "Find a diverse set of relevant documents for a given search topic from a given document collection using a provided search tool.",
        measurement=[ir_measures.alpha_nDCG@20],
        start_offset=0,
        serp_size=20,
    )
```

## Topic Description

- `name`: Name of dataset used by `ir_datasets`
- `type`: Type of dataset. Currently, it only accepts `ir_datasets`
- `topic_class`: Fields of topic file to use (see below)

```python
    topicset=TopicDescription(
        name="aquaint/trec-robust-2005",
        type="ir_datasets",
        topic_class=FullTopic
    )
```

### Topic Class

- `TitleOnlyTopic`: This shows the `title` section of topic files. Use this one when you have a question or query as input.
- `TitleDescriptionTopic`: This shows the `title` and `description` sections of topic files.
- `TitleNarrativeTopic`: This shows the `title` and `narrative` sections of topic files.
- `TitleDescriptionNarrativeTopic` (aka `FullTopic`): This shows the `title`, `description`, and `narrative` sections of topic files.


## Corpus Description

- `name`: Name of the corpus in the test collection
- `descriptoin`: Description of the corpus
- `index_name`: Name of index files generated by opensearch client

```python
    corpus=CorpusDescription(
        name="Aquaint",
        description="A document collection of about 1M English newswire text. Sources include the Xinhua News Service (1996-2000), the New York Times News Service (1998-2000), and the Associated Press Worldstream News Service (1998-2000).",
        index_name="aquaint_bm25",
    )
```

## Model Description

- `type`: LLM types. Currently it accepts `openai`, `azure`, `gemini`, `openrouter`, or `ollama`
- `name`: Name of the model
- `system_prompt`: A system (development) prompt
- `temperature`: Temerature of the model (Default: 0.0)

```python
    models=[
        ModelDescription(
            type="openai",
            name="gpt-4.1-mini-2025-04-14",
            system_prompt="You're a helpful assistant",
            temperature=0.0,
        )
    ]
```

See [Getting Started - LLMs](../getting_started/llms.md) to learn how to configure different LLM types and how to compare them in the experiment.


## Tool Description

Note that `tools` in LLM applications have wide scopes of external applications, which are commonly "called" by LLMs as needed. However, in `geniie-lab` experiments, these tools are "search tools" and called by the experimental design via the `plan` variable, rather than by LLMs.

- `name`: Name of search tool
- `ranking_model`: Name of ranking model used by the tool
- `index_name`: Name of index files used by the tool
- `port`: Port number of opensearch client
- `description`: Description of the tool, query syntax (if any), and ranking model.

```python
    tools=[
        ToolDescription(
            name="opensearch",
            ranking_model="bm25",
            index_name="aquaint_bm25",
            port=9200,
            description="It allows you to perform searches using keywords only and employs the BM25 ranking model to order results.",
        )
    ]
```

See [Getting Started - OpenSearch](../getting_started/opensearch.md) to learn how to configure different ranking models and how to compare them in the experiment.


## Stages

Each stage has a name such as `query`, `ranking`, or `click`, and you can configure the instruction for each of the stages. Note that we do not have an instruction for `ranking` stage since it is executed by the tool without LLMs.

- `instruction`: Instruction given to GII for each of the stages.

```python
    stages={
        "query": StageConfig(
            instruction="""
                Review the provided descriptions of task, corpus, tool and search topic. Then, formulate a search query.
            """,
        ),
        "ranking": StageConfig(
            instruction=""
        ),
        "click": StageConfig(
            instruction="""
                Select a set of documents that are likely to contain relevant information to the search topic. Return an empty list if none of the results appears relevant.
            """,
        ),
        "relevance": StageConfig(
            instruction="""
                Evaluate the relevance of the document based on the search topic description and its narrative.
            """,
        ),
        "reformulate": StageConfig(
            instruction="""
                Formulate another search query to find new relevant documents.
            """,
        ),
    }
```

## Other optional settings

- `max_topics`: Define how many topics in the dataset to be processed in the experiment. If you set to 1, it will execute the first topic (or questions or query) in the dataset. If you set to `None`, the experiment will be run on all topics. Default: `None`
- `full_log`: Define whether or not a full interaction log with LLMs is produced at the end of each topic. Useful for debugging purpose. Make sure to catch STDERR to save the full log. Default: `False`. Alternatively, you can set the log level to `DEBUG` in the logger defined at the beginning of the runner scripts in `scripts` folder.
- `custom_settings`: A variable to store any arbitary strings to note for an experiment (e.g., specific parameter settings). It will be included in the outputs but not to present to GII. Default: `None`

```python
    max_topics=1,
    full_log=False,
    custom_settings=None
```
